{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad6287a",
   "metadata": {},
   "source": [
    "# Final project of Mobile Robotique "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d833c",
   "metadata": {},
   "source": [
    "Student group number 19 : <br/>\n",
    "Kyan Achtari <br/>\n",
    "Louis Gavignet <br/>\n",
    "Louise Genoud <br/>\n",
    "Joaquim Silveira <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a942b6",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<br/>\n",
    "The project of Mobile Robotique aims to combine vision, path planning, global navigation, local navigation, and filtering with a kalman filter to move Thymio to the goal.\n",
    "<br/><br/>\n",
    "We first create a map composed of differents elements : black obstacles where the Thymio must never go, a blue triangle on Thymio to get his position and orientation in real time, a red square goal, and white small obstacles invisible to the camera.  The camera, using the vision modulus, gets the coordinates of every elements on the map from which we extracts a path. We then use the path planning with Dijkstra to extract the best suited path. \n",
    "<br/><br/>\n",
    "While following the path, Thymio may enconters obstacles not detected by the camera. That's why it constantly scans the near environment with the IR sensors, when one is triggered Thymio avoids the obstacles using local navigation.\n",
    "<br/><br/>\n",
    "The robot also use a filter to estimate the position with the help of sensors (camera and motor speed) and to correct it. We choose to implement the kalman filter, which shows a strong efficiency even when the camera is blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a25d73",
   "metadata": {},
   "source": [
    "# Implementaion with vision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c75ed5cb",
   "metadata": {},
   "source": [
    "### Hardware and Software Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "011aaee6",
   "metadata": {},
   "source": [
    "For the computer vision implementation, we will be using the camera METTRE MODEL. The OpenCV library will be used to process and treat the images to our convenience."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d50178fc",
   "metadata": {},
   "source": [
    "### Object recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5f54720",
   "metadata": {},
   "source": [
    "The first step is to differentiate three types of objects: obstacles, the robot and the goal area. They are respectively black, blue and red, can be either squares or triangles, and be placed almost anywhere inside the frame, so long as the obstacles are twice the robot's size a part. The object recognition process for either type consists of the same 4 steps : \n",
    "\n",
    "**1. Convert Image to HSV Color Space** : from research, we found out that the Hue Saturation Value color space is ideal for edge detection, as the distinction between the colors is more evident. Also, it is less sensible to the environment's lightning, as a full Value, equivalent to brightness, corresponds to how the color behaves and appears under a certain lightning. Therefore, full brightness does not mean the color will be white, but it's own color will be more radiant.\n",
    "\n",
    "**2. Smooth the Image**: we smooth the image to reduce noise and make the pixel values inside an object more homogeneous, making it easer to establish an upper and lower color bound (in HSV of course). To smooth the image, we chose to employ the Bilateral Filter proposed by the OpenCV2 library. It is both effective for removing white noise and at the same time keeping edges sharp, which is what we'll need for the next step. Of course, it takes more computing power, but we can compensate this by reducing the image pixel width and height.\n",
    "\n",
    "**3. Image Mask**: To create the object mask, we take the lower and upper color bounds for it, which were measured using the Gimp image processing software, and use the OpenCv2 InRange function, which basically applies a threshold to the image with the given bound. If the pixel is outside of the bound, it's value is set to 0, else it will be set to 1.\n",
    "\n",
    "**4. Object Contour and Center Recognition**: Now with the object isolated in the image, we can detect it's contour and center with the OpenCv2 findContours function, which joins pixel with same values(which is why we apply thresholding, as it will work better to detect contours) and that are near by. Of course, it is highly likely that noise is still present up to this stage, which is why we apply an extra step: we approximate the contour with a simpler shape of roughly the same size, calculate it's area and reject it as a valid object if it is less than a certain threshold. This allows us to ignore smaller features that are big enough to not be picked up by the bilateral smoothing, that may be part of the arena texture. By calculating the moment of the approximated contour, we can find both it's area and center point. \n",
    "\n",
    "We now have all the informations we need for the objects: it's center, it's area and it's edge points. Each step result is shown by the following images:\n",
    "| HSV image | Smooth Image | Image Mask | Object Contour|\n",
    "\n",
    "<img src=\"images/obstacle/img_processed.png\" alt=\"drawing\" width=\"300\"/> <img src=\"images/obstacle/img_hsv.png\" alt=\"drawing\" width=\"300\"/>  <img src=\"images/obstacle/img_smooth.png\" alt=\"drawing\" width=\"300\"/>  \n",
    "<img src=\"images/obstacle/mask.png\" alt=\"drawing\" width=\"300\"/>  <img src=\"images/obstacle/frame.png\" alt=\"drawing\" width=\"300\"/>\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34559934",
   "metadata": {},
   "source": [
    "### Robot Direction Detection\n",
    "Unlike with the obstacles and the goal, we also care about the robot's direction, as we need to know it's orientation to correctly move it through the world. For this, we use an isosceles triangle pointing forward. We calculate it's edges and center as mentionned above, and can measure it's forward orienting vector by finding which vertex of it's contour is further away from the center point. This allows us to calculate it's inclination, seen from above, with respect to the x axis, or the camera's horizon. The forward pointing vector is shown in the image below:\n",
    "\n",
    "<img src=\"\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "METTRE IMAGE ICI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1915e0",
   "metadata": {},
   "source": [
    "# Path planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ec8a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81d61ea9",
   "metadata": {},
   "source": [
    "# Motion control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c440b",
   "metadata": {},
   "source": [
    "### Global navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38388c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78ce9d03",
   "metadata": {},
   "source": [
    "### Local navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4665ca",
   "metadata": {},
   "source": [
    "To avoid the obstacles we setup a PI, which is a vector storing the last 3 seconds of the \"turn\" value :\n",
    "\n",
    "$$ turn = (prox.horizontal[0] //2 + prox.horizontal[1] - prox.horizontal[3] - prox.horizontal[4] //2) // 40 $$\n",
    "\n",
    "The turn value is positive if an obstacle is on the left and negative with an obstacle on the right, taking more into account the middle sensors since it would need to avoid the obstacle more.\n",
    "\n",
    "$$ turn = turn + ki*prox_int $$\n",
    "\n",
    "ki is a vector containing the \"weights\" for the vector of values from the sensors (prox_int). ki contains the values from 1 to 3 times the sampling period, all divided by 20 to take the more recent values more into account. The left wheel is then decelerated by the value of \"turn\", and the right wheel accelerated by it.\n",
    "\n",
    "After 3 seconds if the sensors don't see anything, we return to global navigation instead of local navigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f10df",
   "metadata": {},
   "source": [
    "# Filtering with Kalman filter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d5ab6",
   "metadata": {},
   "source": [
    "### Overview and prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57df6b7a",
   "metadata": {},
   "source": [
    "To calculate the beliefs of the robot position we setup a Bayes filter. It's a recrsive filter taht calculte first a prediction of the robot positioon and then update the measurement. We implemented an extended Kalman filter because we conside a nonlinear gaussian state space model, our state are $X = (x,y, \\theta)$. The states are modified by the speed of the left and right motors U = (speed_left, spped_right). The nonlinear modification of each paramters can be modelled like this : \n",
    "$$ x_{t+1} = x_{t} + v_{average}\\cdot dt\\cdot cos(\\theta) $$ \n",
    "$$ y_{t+1} = y_{t} + v_{average} \\cdot dt\\cdot sin(\\theta) $$ \n",
    "$$ \\theta_{t+1} = \\theta_{t} + dt\\cdot \\omega $$  \n",
    "\n",
    "<c/><c/>\n",
    "We can write the systeme above with some matrices :\n",
    "$$ X_{t+1} = A\\cdot X_{t} + B\\cdot U $$\n",
    "\n",
    "<c/><c/>\n",
    "With the following matrices A and B :\n",
    "$$\n",
    "A = \n",
    "\\left(\\begin{array}{cc} \n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "B = \n",
    "\\left(\\begin{array}{cc} \n",
    "0.5\\cdot dt\\cdot cos(\\theta) & 0.5 \\cdot dt\\cdot cos(\\theta)\\\\\n",
    "0.5\\cdot dt \\cdot sin(\\theta) & 0.5\\cdot dt\\cdot sin(\\theta) \\\\\n",
    "-dt/wheeldist & dt/wheeeldist \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101833ef",
   "metadata": {},
   "source": [
    "The state transition covariance matrix Q, contains the variance of the motors speed :\n",
    "\n",
    "$$ \n",
    "Q = \n",
    "\\left(\\begin{array}{cc}\n",
    "speedvar & 0\\\\\n",
    "0 & speedvar\\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "The covariance of the obervation noise matrix R, errors come from the camera when measuring a known object :\n",
    "\n",
    "$$ \n",
    "R = \n",
    "\\left(\\begin{array}{cc}\n",
    "camvar & 0 & 0\\\\\n",
    "0 & camvar & 0\\\\\n",
    "0 & 0 & camanglevar\\\\\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f788486",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efdfc15a",
   "metadata": {},
   "source": [
    "We can predict the next position of the robot thanks to the set of equations of the nonlinear $X_{t+1}$. We calculate the predication every time interval dt, this periode corresponds to actuation time of our robot. Every dt we calculate the new orientation to follow the path, we update the speed of the motors, and we check with the Kalman filter that Thymio is on the right path.\n",
    "\n",
    "We also compute the prediction of an estimate of the covariance in this function :\n",
    "$P = P+B\\cdot Q\\cdot B^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a9d52",
   "metadata": {},
   "source": [
    "### Update "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc0b72f7",
   "metadata": {},
   "source": [
    "Update the Kalman filter state estimate previously with the Kalman gain K, and the difference between the actual state z and the predicted one x. In the mean time, we update the values of P.\n",
    "\n",
    "$$I = z-X$$\n",
    "and $$K = P \\cdot H^T\\cdot S$$  with $$S=R+H\\cdot P^T\\cdot H$$\n",
    "then $$X=X+K\\cdot I$$\n",
    "and $$P = P-K\\cdot H\\cdot P$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26779ae7",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729027b0",
   "metadata": {},
   "source": [
    "We then implement the filter function by calling the prediction function and the update function in this order every dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e58a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f9566ee846e0e5475f3731207e71ee4a96d604221359f666805a9fa43f54da3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
