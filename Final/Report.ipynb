{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7322126",
   "metadata": {},
   "source": [
    "# Final project of Mobile Robotique "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344c2ef",
   "metadata": {},
   "source": [
    "Student group number 19 : <br/>\n",
    "Kyan Achtari <br/>\n",
    "Louis Gavignet <br/>\n",
    "Louise Genoud <br/>\n",
    "Joaquim Silveira <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4554e838",
   "metadata": {},
   "source": [
    "# 1) Introduction\n",
    "<br/>\n",
    "The project of Mobile Robotique aims to combine vision, path planning, global navigation, local navigation, and filtering with a kalman filter to move Thymio to the goal.\n",
    "<br/><br/>\n",
    "We first create a map composed of differents elements : black obstacles where the Thymio must never go, a blue triangle on Thymio to get his position and orientation in real time, a red square goal, and white small obstacles invisible to the camera.  The camera, using the vision modulus, gets the coordinates of every elements on the map from which we extracts a path. We then use the path planning with Dijkstra to extract the best suited path. \n",
    "<br/><br/>\n",
    "While following the path, Thymio may enconters obstacles not detected by the camera. That's why it constantly scans the near environment with the IR sensors, when one is triggered Thymio avoids the obstacles using local navigation.\n",
    "<br/><br/>\n",
    "The robot also use a filter to estimate the position with the help of sensors (camera and motor speed) and to correct it. We choose to implement the kalman filter, which shows a strong efficiency even when the camera is blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e1cb1",
   "metadata": {},
   "source": [
    "# 2) Computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6810ab50",
   "metadata": {},
   "source": [
    "### Hardware and Software Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eedf66",
   "metadata": {},
   "source": [
    "For the computer vision implementation, we will be using the camera METTRE MODEL. The OpenCV library will be used to process and treat the images to our convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cba233",
   "metadata": {},
   "source": [
    "METTRE PHOTO DE LA CAMERA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34fe2dd",
   "metadata": {},
   "source": [
    "### Object recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f04046",
   "metadata": {},
   "source": [
    "The first step is to differentiate three types of objects: obstacles, the robot and the goal area. They are respectively black, blue and red, can be either squares or triangles, and be placed almost anywhere inside the frame, so long as the obstacles are twice the robot's size a part. The object recognition process for either type consists of the same 4 steps : \n",
    "\n",
    "**1. Convert Image to HSV Color Space** : from research, we found out that the Hue Saturation Value color space is ideal for edge detection, as the distinction between the colors is more evident. Also, it is less sensible to the environment's lightning, as a full Value, equivalent to brightness, corresponds to how the color behaves and appears under a certain lightning. Therefore, full brightness does not mean the color will be white, but it's own color will be more radiant.\n",
    "\n",
    "**2. Smooth the Image**: we smooth the image to reduce noise and make the pixel values inside an object more homogeneous, making it easer to establish an upper and lower color bound (in HSV of course). To smooth the image, we chose to employ the Bilateral Filter proposed by the OpenCV2 library. It is both effective for removing white noise and at the same time keeping edges sharp, which is what we'll need for the next step. Of course, it takes more computing power, but we can compensate this by reducing the image pixel width and height.\n",
    "\n",
    "**3. Image Mask**: To create the object mask, we take the lower and upper color bounds for it, which were measured using the Gimp image processing software, and use the OpenCv2 InRange function, which basically applies a threshold to the image with the given bound. If the pixel is outside of the bound, it's value is set to 0, else it will be set to 1.\n",
    "\n",
    "**4. Object Contour and Center Recognition**: Now with the object isolated in the image, we can detect it's contour and center with the OpenCv2 findContours function, which joins pixel with same values(which is why we apply thresholding, as it will work better to detect contours) and that are near by. Of course, it is highly likely that noise is still present up to this stage, which is why we apply an extra step: we approximate the contour with a simpler shape of roughly the same size, calculate it's area and reject it as a valid object if it is less than a certain threshold. This allows us to ignore smaller features that are big enough to not be picked up by the bilateral smoothing, that may be part of the arena texture. By calculating the moment of the approximated contour, we can find both it's area and center point. \n",
    "\n",
    "We now have all the informations we need for the objects: it's center, it's area and it's edge points. Each step result is shown by the following images:\n",
    "| HSV image | Smooth Image | Image Mask | Object Contour|\n",
    "\n",
    "<img src=\"images/obstacle/img_processed.png\" alt=\"drawing\" width=\"300\"/> <img src=\"images/obstacle/img_hsv.png\" alt=\"drawing\" width=\"300\"/>  <img src=\"images/obstacle/img_smooth.png\" alt=\"drawing\" width=\"300\"/>  \n",
    "<img src=\"images/obstacle/mask.png\" alt=\"drawing\" width=\"300\"/>  <img src=\"images/obstacle/frame.png\" alt=\"drawing\" width=\"300\"/>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a08f7",
   "metadata": {},
   "source": [
    "### Robot Direction Detection\n",
    "Unlike with the obstacles and the goal, we also care about the robot's direction, as we need to know it's orientation to correctly move it through the world. For this, we use an isosceles triangle pointing forward. We calculate it's edges and center as mentionned above, and can measure it's forward orienting vector by finding which vertex of it's contour is further away from the center point. This allows us to calculate it's inclination, seen from above, with respect to the x axis, or the camera's horizon. The forward pointing vector is shown in the image below:\n",
    "\n",
    "<img src=\"images/path.png\" alt=\"drawing\" height=\"350\"/><img src=\"images/dir.png\" alt=\"drawing\" height=\"350\"/>\n",
    "\n",
    "This is done with the following function:\n",
    "```\n",
    "def get_robot_position(frame, robot_center, robot_contour):\n",
    "    center = np.array(robot_center, dtype=\"object\")\n",
    "    contour = np.array(robot_contour, dtype=\"object\")\n",
    "    center = np.reshape(np.ravel(center), (-1,2))\n",
    "    contour = np.reshape(np.ravel(contour), (-1,2)) \n",
    "    \n",
    "    min_dist = 0\n",
    "    max_index = 0\n",
    "    dir_vector = alpha = 0\n",
    "    for i in range(len(contour)):\n",
    "        if euclidean(contour[i], center[0]) > min_dist:\n",
    "            min_dist = euclidean(contour[i], center[0])\n",
    "            max_index = i\n",
    "    dir_vector = contour[max_index] - center[0]\n",
    "    \n",
    "    alpha = np.arctan2(dir_vector[1], dir_vector[0])\n",
    "    cv2.arrowedLine(frame, center[0], contour[max_index], (0, 0, 255), 2)\n",
    "\n",
    "    return dir_vector, alpha\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013aa5b",
   "metadata": {},
   "source": [
    "### Computer Vision Step by Step\n",
    "\n",
    "The following function is responsible for starting up the computer vision side of the project, detecting all the different object and providing the needed data for the Path Planning algorithm to execute properly:\n",
    "```\n",
    "def cv_start(video_capture, exposure = None, show_image = False, nb_tries = 5):\n",
    "```\n",
    "It does the following, in order:\n",
    "\n",
    "1. **Setup the Camera** : starts the camera, ignores the first 300 frames, as the camera is still adjusting to the environment lightning, and sets it's exposure. This last parameter can be ignored, and the camera will set the exposure automatically. We choose to adjust it so it is easier to change our testing environment.\n",
    "2. **Detect Obstacles, Robot and Goal Object** : We execute the object detection function to detect the three types of objects. We do this a few times, up until either all three types are detected or the maximum number of tries has been reached, so as to avoid infite loops.\n",
    "3. **Robot Position and Orientation** : Finally, we get the robot position and orientation.\n",
    "\n",
    "It returns the followin data:\n",
    "\n",
    " - *cv_successful* : True if all types of objects were detected, false if either the latter is not true or the maximum number of tries has been reached\n",
    " - *obst_contours* : (x,y) position of obstacle vertexes. Useful for the Path Planning algorithm.\n",
    " - *robot_pos* : (x,y) position of the center of the robot.\n",
    " - *goal_center* :  (x,y) position of the goal area.\n",
    " - *frame* : frame treated, with the drawn contours around each object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2686a",
   "metadata": {},
   "source": [
    "# 3) Path planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f331bf6",
   "metadata": {},
   "source": [
    "The vision module returns a list of obstacles, characterized by the coordinates of their vertices. From these points, the path planning module does several things :\n",
    "\n",
    "- **Obstacle augmentation** : construct a series of points through which the Thymio can go, from the coordinates of the obstacle vertices\n",
    "- **Graph construction** : build a graph structure with the right connections between vertices (so that Thymio cannot cross an obstacle) and the weights of the edges initialized (they correspond to th distances between the points)\n",
    "- **Shortest path** : using Dijkstra's algorithm, find the shortest path from the start point to the end point, passing through a subset of the vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aaedd1",
   "metadata": {},
   "source": [
    "### Obstacle augmentation\n",
    "The obstacle augmentation is done by moving each vertex of each obstacle in two different ways, as illustrated in the figure below for vertex A :  \n",
    "\n",
    "<img src=images/obstacle.JPG width=\"400\">\n",
    "\n",
    "The point is moved in the direction parallel to each edge of the obstacle, and then in the perpendicular direction away from the obstacle. The distance of each of these translation is a constant defined as half the Thymio width plus a safety margin. This is done so that the robot will not collide with the corner of the obstacle when going around it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbfe01a",
   "metadata": {},
   "source": [
    "### Graph construction\n",
    "The construction of the graph from the augmented vertices is done in 2 steps for each obstacle :\n",
    "- The consecutive vertices are connected to each other (yellow edges)\n",
    "- The vertex of the obstacle that is the closest to a vertex of another obstacle is connected to that vertex (red edge)\n",
    "\n",
    "<img src=images/path_around_sev.JPG width=\"800\">  \n",
    "\n",
    "The start point (given by the robot's initial position) and end point (target) are each connected to the vertex of the graph that is the closest to them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec7e86",
   "metadata": {},
   "source": [
    "### Shortest path\n",
    "The shortest path can be computed using Dijkstra's algorithm on the graph that was constructed. First, the table of _distances_ is initialized with all distances to a value INF representing infinity, and the table of _predecessors_ is created. An _unconnected_ list is also created containing the indexes of all the vertices at the beginning.  \n",
    "\n",
    "The _graph_ is a list of lists, with every vertex represented by a list of its connected vertices and the weight of the edge connecting them. \n",
    "These tables are then updated as such :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf3fa7",
   "metadata": {},
   "source": [
    "```\n",
    "    while(len(unconnected)!=0):\n",
    "    dp = distances.copy()[unconnected]\n",
    "    idx_a = np.argmin(dp,axis=0)[0]\n",
    "    a = int(dp[idx_a,1])\n",
    "    remove(a,unconnected)\n",
    "\n",
    "    for v_b in graph[a]:\n",
    "        b = v_b[0]\n",
    "\n",
    "        w_ab = get_weight(a,b,graph)\n",
    "        if((w_ab > 0) and (distances[b,0]> distances[a,0] + w_ab)):\n",
    "            distances[b,0] = distances[a,0] + w_ab\n",
    "            predecessor[b] = a\n",
    "\n",
    "\n",
    "return distances,predecessor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252cb3b",
   "metadata": {},
   "source": [
    "From the resulting list of predecessors, the path (_coord_) can then be extracted by walking through it from end to beginning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6e751",
   "metadata": {},
   "source": [
    "```\n",
    "coord.append(end)\n",
    "\n",
    "while(pred[i] != len(pred)-2):\n",
    "    i = pred[i]\n",
    "    path_num.append(i)\n",
    "    coord.append(coordlist[i][0:2])\n",
    "\n",
    "coord.append(start)\n",
    "coord.reverse()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e50f1",
   "metadata": {},
   "source": [
    "# 4) Motion control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc1f530",
   "metadata": {},
   "source": [
    "### Global navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46638d6f",
   "metadata": {},
   "source": [
    "##### Implementation of the movement\n",
    "We create a fonction called pathing() that uses all the functions needed for moving the Thymio. In every iteration, pathing calls the camera, kalman, and refers to the path given by the path planning. The Thymio moves continuously and actuates the speed of his motors thanks to a proportionnal controller.\n",
    "\n",
    "##### Rotation \n",
    "When the Thymio arrives at an intermediate goal, he has to change his orientation to go to the next one. The function get_angle_between() gives the angular difference between the two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac63597",
   "metadata": {},
   "source": [
    "```\n",
    "def get_angle_between(vec1, vec2):\n",
    "    vec1_unit = vec1 / np.linalg.norm(vec1)\n",
    "    vec2_unit = vec2 / np.linalg.norm(vec2)\n",
    "\n",
    "    return np.arccos(np.dot(vec1_unit, vec2_unit))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d0d16",
   "metadata": {},
   "source": [
    "Then, we make sure the angle belongs to the interval [-$\\pi$, $\\pi$] :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45af98",
   "metadata": {},
   "source": [
    "```\n",
    "def wrap_angle(angle):\n",
    "    if angle > math.pi:\n",
    "        angle = angle - 2*math.pi\n",
    "    elif angle < -math.pi:\n",
    "        angle = angle + 2*math.pi\n",
    "    return angle\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f72e1",
   "metadata": {},
   "source": [
    "##### Controller\n",
    "\n",
    "We use a proportionnal controller to set the speed of the motors. It takes into account the angle for the correction or the new orientation. We also set an angle minimum under which the robot won't turn, the kalman correction can be very precice so we don't take it in consideration every time. This angle minimum is equal to 0.4 radians, which corresponds to 23 degrees. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec57cf0",
   "metadata": {},
   "source": [
    "```\n",
    "def controller(angle)\n",
    "    if abs(angle) > ANGLE_TOLERANCE:\n",
    "                speed_l = 80 - kp_rot*(angle)\n",
    "                speed_r = 80 + kp_rot*(angle)\n",
    "            else:\n",
    "                speed_l = SPEED_AVG\n",
    "                speed_r = SPEED_AVG\n",
    "    return speed_l, speed_r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8cb71",
   "metadata": {},
   "source": [
    "### Local navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30344a3",
   "metadata": {},
   "source": [
    "We read the 7 horizontal sensor values and store them in prox.vals and multiply them by IR.SCALE.DOWN (which is 0.05), if any of the values is non-zero (other than the last 2, which are behind the Thymio), we enter obstacle avoidance:\n",
    "\n",
    "Then we multiply the vector prox.vals by LEFT.IR.WEIGHT and RIGHT.IR.WEIGHT for each wheel:\n",
    "\n",
    "$$\n",
    "LEFT.IR.WEIGHT =\n",
    "\\left(\\begin{array}{cc} \n",
    "1 \\\\\n",
    "2 \\\\\n",
    "-5 \\\\\n",
    "-2 \\\\\n",
    "-1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{array}\\right)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "RIGHT.IR.WEIGHT =\n",
    "\\left(\\begin{array}{cc} \n",
    "-1 \\\\\n",
    "-2 \\\\\n",
    "-5 \\\\\n",
    "2 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{array}\\right)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "$$\n",
    "\n",
    "The speed of each wheel is their respective weight multiplied by the sensor values, and + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b869c",
   "metadata": {},
   "source": [
    "```\n",
    "if sum(prox_vals[0], prox_vals[1], prox_vals[2], prox_vals[3], prox_vals[4]) > 0:\n",
    "    obst = True\n",
    "\n",
    "if obst == True:\n",
    "    speed_l = sum(prox_vals * LEFT_IR_WEIGHT)  + 10\n",
    "    speed_r = sum(prox_vals * RIGHT_IR_WEIGHT) + 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab172a1d",
   "metadata": {},
   "source": [
    "# 5) Kalman filter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dea95",
   "metadata": {},
   "source": [
    "### Overview and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6295121",
   "metadata": {},
   "source": [
    "To calculate the beliefs of the robot position we setup a Bayes filter. It's a recursive filter that first calcultes a prediction of the robot positioon and then update the measurement. We implemented an extended Kalman filter because we conside a nonlinear gaussian state space model, as our physical model includes cosinusoidales factors. Our states are $X = (x,y, \\theta)$. The states are modified by the speed of the left and right motors $U = (speed.left, spped.right)$. The nonlinear modification of each paramters can be modeled as follows : \n",
    "\n",
    "$$ x_{t+1} = x_{t} + v_{average}\\cdot dt\\cdot cos(\\theta) $$ \n",
    "$$ y_{t+1} = y_{t} + v_{average} \\cdot dt\\cdot sin(\\theta) $$ \n",
    "$$ \\theta_{t+1} = \\theta_{t} + dt\\cdot \\omega $$  \n",
    "\n",
    "<c/><c/>\n",
    "We can write the systeme above with the following matrices :\n",
    "$$ X_{t+1} = A\\cdot X_{t} + B\\cdot U $$\n",
    "\n",
    "<c/><c/>\n",
    "With the following matrices A and B :\n",
    "$$\n",
    "A = \n",
    "\\left(\\begin{array}{cc} \n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "B = \n",
    "\\left(\\begin{array}{cc} \n",
    "0.5\\cdot dt\\cdot cos(\\theta) & 0.5 \\cdot dt\\cdot cos(\\theta)\\\\\n",
    "0.5\\cdot dt \\cdot sin(\\theta) & 0.5\\cdot dt\\cdot sin(\\theta) \\\\\n",
    "-dt/wheeldist & dt/wheeeldist \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8e7aa",
   "metadata": {},
   "source": [
    "The state transition covariance matrix Q, contains the variance of the motors speed :\n",
    "\n",
    "$$ \n",
    "Q = \n",
    "\\left(\\begin{array}{cc}\n",
    "speedvar & 0\\\\\n",
    "0 & speedvar\\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "The covariance of the obervation noise matrix R, errors come from the camera when measuring a known object :\n",
    "\n",
    "$$ \n",
    "R = \n",
    "\\left(\\begin{array}{cc}\n",
    "camvar & 0 & 0\\\\\n",
    "0 & camvar & 0\\\\\n",
    "0 & 0 & camanglevar\\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "It is a diagonal matrix as the $x$ position, $y$ position and $theta$ angle are independant variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129c89f",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec0f845",
   "metadata": {},
   "source": [
    "We can predict the next position of the robot thanks to the set of equations of the nonlinear $X_{t+1}$. We calculate the prediction at every time interval $dt$, this periode corresponds to reaction and update time of our robot. At each $dt$ we calculate the new orientation to follow the path, we update the speed of the motors, and we check with the Kalman filter that the Thymio is on the right path.\n",
    "\n",
    "We also compute the prediction of an estimate of the covariance with the following expression:\n",
    "$P = P+B\\cdot Q\\cdot B^T$\n",
    "The Prediction code is the following:\n",
    "```\n",
    "def kalman_predict(previous_time, x, u, P):  \n",
    "        start_time = time.time()\n",
    "            \n",
    "        if (previous_time != 0):\n",
    "            dt = round(start_time - previous_time, 8)\n",
    "        else:  \n",
    "            dt = 0.2\n",
    "        \n",
    "        states_dim = len(x)     # x, y, theta\n",
    "        control_dim = len(u)\n",
    "        \n",
    "        A = np.eye(states_dim)\n",
    "        B = np.array([[0.5 * dt * np.cos(x[2]),     0.5 * dt * np.cos(x[2])], \n",
    "                      [0.5 * dt * np.sin(x[2]),     0.5 * dt * np.sin(x[2])],\n",
    "                      [ -dt / ROBOT_LENGTH,          dt / ROBOT_LENGTH]], dtype='float')\n",
    " \n",
    "        x = B.dot(u) + A.dot(x) \n",
    "        Q = SPEED_VAR * np.eye(control_dim)\n",
    "        P = B.dot(Q).dot(B.T) + P\n",
    "\n",
    "        return start_time, x, P\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4e1b2",
   "metadata": {},
   "source": [
    "### Update "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa78b71a",
   "metadata": {},
   "source": [
    "Once we get a measurement of the states, with the camera in our case, we update the Kalman filter state estimation with the previously calculated Kalman Gain $K$, the measurement of the actual state ¨$z$ and the predicted state $x$. Then we update the values of the $P$ matrix with the following expressions:\n",
    "\n",
    "$$I = z-X$$\n",
    "and $$K = P \\cdot H^T\\cdot S$$  with $$S=R+H\\cdot P^T\\cdot H$$\n",
    "then $$X=X+K\\cdot I$$\n",
    "and $$P = P-K\\cdot H\\cdot P$$ \n",
    "\n",
    "The Update code is the following: \n",
    "```\n",
    "def kalman_update(x, z, P, sensor_available):\n",
    "    R = np.diag([CAMERA_VAR, CAMERA_VAR, CAMERA_ANGLE_VAR])\n",
    "    states_dim = len(x) \n",
    "    \n",
    "    if sensor_available : H = np.eye(states_dim)\n",
    "    else : H = np.zeros((states_dim, states_dim))\n",
    "\n",
    "    I = z - x\n",
    "    S = H.dot(P).dot(H.T)  + R\n",
    "    K_gain = P.dot(H.T).dot(np.linalg.inv(S)) \n",
    "    x = x + K_gain.dot(I)\n",
    "    P = P - K_gain.dot(H).dot(P)\n",
    "\n",
    "    return x, P  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16071266",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899f383",
   "metadata": {},
   "source": [
    "The whole filtering processes, when the camera detects the robot, involves both Prediction and Update steps. However, on the abscence of measurements, we soly rely on the Prediction, as we have no measurements to update our beliefs. Kalman is implemented with the following function:\n",
    "\n",
    "```\n",
    "\n",
    "def kalman_filter(sensor_data_available, x, u, z, P , previous_time):\n",
    "\n",
    "    next_time, x_kal, P  = kalman_predict(previous_time, x, u, P)\n",
    "    x_predicted = x_kal\n",
    "    if sensor_data_available == True :\n",
    "        x_kal, P = kalman_update(x, z, P, sensor_data_available)\n",
    "\n",
    "    return next_time, x_kal, P, x_predicted\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff23ec",
   "metadata": {},
   "source": [
    "### Calibration\n",
    "In total, we need to calibrate the following parameters:\n",
    "- *speedvar* : variance of the motor's speed. We use the values determined in Exercice 8 to determine it experimentally. The Covariance matrix $Q$ will be diagonal as the left and right speed are independant.\n",
    "- *camvar* : position measurement noise of the camera. We again assume quite fairly that $x$ and $y$ noise are independant. For simplicity, we'll stretch this assumption to include $theta$ into the mix of independance. We measured the position of the robot and it's orientation, and most of the time, the position was inside a 2.5 pixel radius, which gives us a 0.05 variance.\n",
    "- *camvarangle* : the orientation noise is given by the (x,y) position noise propagation in both axis, so it gives us $0.0025$.\n",
    "\n",
    "This results where tested, with the following results:\n",
    "\n",
    "<img src=images/kalman/tresbonkalman.png width=\"700\">  \n",
    "\n",
    "The filter worked as expected, joining the camera measurement once the robot was detectable again by the camera quite precisely. Other cases are shown below:\n",
    "\n",
    "<img src=images/kalman/kalman.png width=\"700\">  \n",
    "<img src=images/kalman/kalman_good.png width=\"700\">  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bba77e",
   "metadata": {},
   "source": [
    "# 6) Overview and  Project Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88cc1e1",
   "metadata": {},
   "source": [
    "### Overview\n",
    "The robot's behaviour is depicted by the following general FSM: \n",
    "\n",
    "<img src=images/FSM.jpg width=\"700\">  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158807d9",
   "metadata": {},
   "source": [
    "### Step 1 : Includes and Functions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8accdd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkalman\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtdmclient\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnotebook\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[39mawait\u001b[39;00m tdmclient\u001b[39m.\u001b[39mnotebook\u001b[39m.\u001b[39mstart()\n\u001b[0;32m     12\u001b[0m \u001b[39m# parametres\u001b[39;00m\n\u001b[0;32m     13\u001b[0m stop \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\osour\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tdmclient\\notebook\\private.py:107\u001b[0m, in \u001b[0;36mstart\u001b[1;34m(zeroconf, zeroconf_all, tdm_ws, tdm_addr, tdm_port, password, debug, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mstart\u001b[39m(zeroconf\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, zeroconf_all\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     91\u001b[0m                 tdm_ws\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, tdm_addr\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tdm_port\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     92\u001b[0m                 password\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     93\u001b[0m                 debug\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     94\u001b[0m     \u001b[39m\"\"\"Start the connection with the Thymio and variable synchronization.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \n\u001b[0;32m     96\u001b[0m \u001b[39m    Arguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m        zeroconf_all - True to use zeroconf with all interfaces instead of default (default: false)\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     client \u001b[39m=\u001b[39m ClientAsync(zeroconf\u001b[39m=\u001b[39;49mzeroconf, zeroconf_all\u001b[39m=\u001b[39;49mzeroconf_all,\n\u001b[0;32m    108\u001b[0m                          tdm_addr\u001b[39m=\u001b[39;49mtdm_addr, tdm_port\u001b[39m=\u001b[39;49mtdm_port, tdm_ws\u001b[39m=\u001b[39;49mtdm_ws,\n\u001b[0;32m    109\u001b[0m                          password\u001b[39m=\u001b[39;49mpassword,\n\u001b[0;32m    110\u001b[0m                          debug\u001b[39m=\u001b[39;49mdebug)\n\u001b[0;32m    111\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m client\u001b[39m.\u001b[39mwait_for_node(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    112\u001b[0m     \u001b[39mawait\u001b[39;00m node\u001b[39m.\u001b[39mlock()\n",
      "File \u001b[1;32mc:\\Users\\osour\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tdmclient\\clientasync.py:42\u001b[0m, in \u001b[0;36mClientAsync.__init__\u001b[1;34m(self, node_class, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, node_class\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[39msuper\u001b[39m(ClientAsync, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     43\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_class \u001b[39m=\u001b[39m node_class \u001b[39mor\u001b[39;00m tdmclient\u001b[39m.\u001b[39mClientAsyncCacheNode\n",
      "File \u001b[1;32mc:\\Users\\osour\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tdmclient\\client.py:98\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, zeroconf, zeroconf_all, tdm_ws, tdm_addr, tdm_port, tdm_transport, password, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_transport \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTDM \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_addr\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_port\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m     99\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend_handshake(password)\n",
      "File \u001b[1;32mc:\\Users\\osour\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tdmclient\\client.py:112\u001b[0m, in \u001b[0;36mClient.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm \u001b[39m=\u001b[39m TDMConnectionWS(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_addr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_ws_port)\n\u001b[0;32m    111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm \u001b[39m=\u001b[39m TDMConnection(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtdm_addr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtdm_port)\n",
      "File \u001b[1;32mc:\\Users\\osour\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tdmclient\\tcp.py:104\u001b[0m, in \u001b[0;36mTDMConnection.__init__\u001b[1;34m(self, host, port, debug)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[0;32m    102\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39msendall(b)\n\u001b[1;32m--> 104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mio \u001b[39m=\u001b[39m TCPClientIO(host, port)\n\u001b[0;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m=\u001b[39m debug\n\u001b[0;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\osour\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tdmclient\\tcp.py:96\u001b[0m, in \u001b[0;36mTDMConnection.__init__.<locals>.TCPClientIO.__init__\u001b[1;34m(self, host, port)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, host, port):\n\u001b[0;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m socket\u001b[39m.\u001b[39msocket(socket\u001b[39m.\u001b[39mAF_INET, socket\u001b[39m.\u001b[39mSOCK_STREAM)\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((host, port))\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "from computer_vision import *\n",
    "from dijkstra import compute_shortest_path\n",
    "from kalman import *\n",
    "\n",
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()\n",
    "# parametres\n",
    "stop = 0\n",
    "MIN_DIST = 25\n",
    "MIN_OBST_DIST = 1000\n",
    "ANGLE_TOLERANCE = 0.40\n",
    "SPEED_AVG = 200\n",
    "ROBOT_SPEED_TO_MM = 120/500\n",
    "IR_SCALE_DOWN = 0.05\n",
    "\n",
    "LEFT_IR_WEIGHT = [1,2,5,-2,-1,0,0]\n",
    "RIGHT_IR_WEIGHT = [-1,-2,-5,2,1,0,0]\n",
    "\n",
    "period = 0.25\n",
    "nb_samples = int(3/period)\n",
    "prox_int = np.zeros((1,nb_samples))\n",
    "state = 'tracking'\n",
    "timer = 0\n",
    "\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def prox():\n",
    "    global prox_horizontal\n",
    "    # get the prox values   \n",
    "    prox_vals = np.array(prox_horizontal) * IR_SCALE_DOWN\n",
    "    return prox_vals\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def motors(l_speed=500, r_speed=500):\n",
    "    global motor_left_target, motor_right_target\n",
    "    \n",
    "    motor_left_target = l_speed\n",
    "    motor_right_target = r_speed\n",
    "\n",
    "def get_angle_between(vec1, vec2):\n",
    "    vec1_unit = vec1 / np.linalg.norm(vec1)\n",
    "    vec2_unit = vec2 / np.linalg.norm(vec2)\n",
    "\n",
    "    return np.arccos(np.dot(vec1_unit, vec2_unit))\n",
    "\n",
    "def distance (x1, y1, x2, y2):\n",
    "    dist = np.sqrt((x1-x2)**2 + (y1-y2)**2)\n",
    "    return dist\n",
    "\n",
    "def wrap_angle(angle):\n",
    "    if angle > math.pi:\n",
    "        angle = angle - 2*math.pi\n",
    "    elif angle < -math.pi:\n",
    "        angle = angle + 2*math.pi\n",
    "    return angle\n",
    "\n",
    "def controller(angle):\n",
    "    kp_rot = 100\n",
    "    kp_lin = 3\n",
    "    obst = False\n",
    "\n",
    "    prox_vals = prox()\n",
    "    if sum(prox_vals) > 0:\n",
    "        obst = True\n",
    "    if obst == True:\n",
    "        print(sum(prox_vals * LEFT_IR_WEIGHT), sum(prox_vals * RIGHT_IR_WEIGHT))\n",
    "        speed_l = sum(prox_vals * LEFT_IR_WEIGHT)  + 10\n",
    "        speed_r = sum(prox_vals * RIGHT_IR_WEIGHT) + 10\n",
    "    else:\n",
    "        if abs(angle) > ANGLE_TOLERANCE:\n",
    "            speed_l = 80 - kp_rot*(angle)\n",
    "            speed_r = 80 + kp_rot*(angle)\n",
    "        else:\n",
    "            speed_l = SPEED_AVG\n",
    "            speed_r = SPEED_AVG\n",
    "    return int(speed_l), int(speed_r)\n",
    "\n",
    "def pathing (video_capture, path, x0, y0, theta0):\n",
    "    global period\n",
    "    cam_hist = []\n",
    "    kal_hist = []\n",
    "    pred_hist = []\n",
    "\n",
    "    teta = np.zeros(np.shape(path)[0])\n",
    "    speed_l = speed_r = 0\n",
    "    goalx = path[1][0]\n",
    "    goaly = path[1][1]\n",
    "    theta_measure = 0\n",
    "    i = 1\n",
    "    curr_time = 0\n",
    "    kx = kteta = ky = angle = 0\n",
    "    robot_detected = False\n",
    "    camera_available = False\n",
    "    dist = distance (goalx, goaly, kx, ky)\n",
    "    \n",
    "    # kalman variables \n",
    "    P = 1000*np.eye(3)\n",
    "    x = x_predicted = np.array([[x0],[y0],[theta0]])\n",
    "    z = np.array([[0],[0],[0]])\n",
    "    u = np.array([[0],[0]])\n",
    "    previous_time = 0\n",
    "    \n",
    "    previous_time, x, P, x_predicted = kalman_filter(True, x, u, z, P , previous_time)\n",
    "\n",
    "    while True:       \n",
    "        #   step 1: get the image, detect the robot, and get the position\n",
    "        current_time = time.time()\n",
    "        camera_available, frame = video_capture.read()\n",
    "        position_measure = []\n",
    "        if camera_available == True:\n",
    "            position_measure, position_contour, proccessed_framed = computer_vision(frame, 'robot', False)\n",
    "            # cv has detected the robot, we can use the position\n",
    "            if len(position_measure) == 1:\n",
    "                robot_detected = True\n",
    "                position_measure = position_measure[0]\n",
    "\n",
    "                if position_measure is not None:                    \n",
    "                    _, theta_measure = get_robot_position(frame, position_measure, position_contour)\n",
    "                    theta_measure = round(-theta_measure, 4)\n",
    "                    position_measure = invert_coordinates(position_measure)\n",
    "                    position_measure = pixel_to_metric(position_measure)\n",
    "                    [kx, ky, kteta] = [position_measure[0], position_measure[1], theta_measure]\n",
    "                    measurement = np.array([kx, ky, kteta])\n",
    "                else:\n",
    "                    robot_detected = False\n",
    "            else:\n",
    "            # robot not detected, we use kalman\n",
    "                robot_detected = False\n",
    "        vals = prox()\n",
    "        \n",
    "        print(vals)\n",
    "        if camera_available == False or robot_detected == False:\n",
    "            measurement = np.array([0,0,0])         \n",
    "        \n",
    "        z = np.array([[measurement[0]], [measurement[1]], [measurement[2]]])\n",
    "        previous_time, x, P, x_predicted = kalman_filter(robot_detected, x, u, z, P , previous_time)\n",
    "        [pred_x, pred_y, pred_teta] = x\n",
    "        \n",
    "        \n",
    "        dist = distance (goalx, goaly, pred_x, pred_y)\n",
    "        teta[i] = math.atan2((goaly - pred_y), (goalx - pred_x)) \n",
    "    \n",
    "\n",
    "        if (dist < MIN_DIST):\n",
    "            if i < np.shape(path)[0] - 1 :\n",
    "                i = i + 1\n",
    "                goalx = path[i][0]\n",
    "                goaly = path[i][1]\n",
    "            else:\n",
    "                motors(stop, stop)\n",
    "                break\n",
    "        else :\n",
    "            angle =  wrap_angle((teta[i] - pred_teta) % (2*np.pi))\n",
    "\n",
    "            speed_l, speed_r = controller(angle)\n",
    "   \n",
    "        u = np.array([[speed_l],[speed_r]]) * ROBOT_SPEED_TO_MM\n",
    "       \n",
    "        motors(speed_l, speed_r)\n",
    "\n",
    "        cam_hist.append([kx, ky, kteta])\n",
    "        kal_hist.append([pred_x, pred_y, pred_teta])\n",
    "        pred_hist.append([goalx, goaly, teta[i]])\n",
    "        end_time = time.time()\n",
    "        period = end_time - current_time\n",
    "        \n",
    "        print(i,period, robot_detected, dist, pred_x, pred_y, pred_teta, angle)\n",
    "\n",
    "    motors(stop, stop)\n",
    "    return cam_hist, kal_hist, pred_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f90d8",
   "metadata": {},
   "source": [
    "### Step 2 : Computer Vision and Global Navigation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0,cv2.CAP_DSHOW)\n",
    "cv_successful, obst, robot, goal, frame = cv_start(video_capture, show_image= True, exposure=-7)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "obst = format_contour(obst)\n",
    "\n",
    "if cv_successful:\n",
    "    start = np.array([robot[0], robot[1]])\n",
    "    goal = np.array([goal[0], goal[1]])\n",
    "    path = compute_shortest_path(obst, start, goal)\n",
    "    path = np.rint(path).astype(int)\n",
    "    frame = draw_path(frame, path) \n",
    "        \n",
    "    metric_path = np.zeros(np.shape(path))\n",
    "\n",
    "    # invert y axis\n",
    "    for i in range(len(path)):\n",
    "        path[i] = invert_coordinates(path[i])\n",
    "    metric_start = invert_coordinates(start)\n",
    "    metric_goal = invert_coordinates(goal)\n",
    "\n",
    "    # change to metric  \n",
    "\n",
    "    for i in range(len(path)):\n",
    "        metric_path[i] = pixel_to_metric(path[i])\n",
    "    metric_start = pixel_to_metric(metric_start)\n",
    "    metric_goal = pixel_to_metric(metric_goal)\n",
    "    \n",
    "    \n",
    "    print('start =', metric_start, 'goal =', metric_goal, 'angle =', -robot[2])\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce512763",
   "metadata": {},
   "source": [
    "### Step 3 : Pathing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf81896",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam, kal, pred = pathing(video_capture, metric_path, metric_start[0], metric_start[1], -robot[2])\n",
    "print(\"finished\")\n",
    "motors(stop, stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b235c29",
   "metadata": {},
   "source": [
    "### Step 4 : Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5118e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_copy = np.array(cam)\n",
    "kal_copy = np.array(kal)\n",
    "pred_copy = np.array(pred)\n",
    "obst_copy  = np.array(obst)\n",
    "\n",
    "#plot \n",
    "pixel_cam = np.zeros(np.shape(cam_copy))\n",
    "pixel_kal = np.zeros(np.shape(kal_copy))\n",
    "pixel_pred = np.zeros(np.shape(pred_copy))\n",
    "# transform to pixel\n",
    "for i in range(len(cam)):\n",
    "    pixel_cam[i] = metric_to_pixel(cam_copy[i])\n",
    "    pixel_kal[i] = metric_to_pixel(kal_copy[i])\n",
    "    pixel_pred[i] = metric_to_pixel(pred_copy[i])\n",
    "\n",
    "implot = plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), origin='upper', alpha=0.5)\n",
    "plt.plot(pixel_kal[:,0], -pixel_kal[:,1] + CAMERA_HEIGHT, 'r', label='camera', linestyle='--', linewidth=0.5)\n",
    "plt.plot(pixel_cam[:,0], -pixel_cam[:,1]+ CAMERA_HEIGHT, 'b', label='kalman', linestyle='--', linewidth=0.5)\n",
    "plt.plot(pixel_pred[:,0], -pixel_pred[:,1]+ CAMERA_HEIGHT, 'g', label='predicted', linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3312c06",
   "metadata": {},
   "source": [
    "# 7) Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68326263",
   "metadata": {},
   "source": [
    "This project ended up being quite challenging, not so much for each individual block composing it, but more so due to their integration. Errors could come from any one parameter changed, that impacted everything else, be it the performance or the precision. We did not have enough time to sharpen and improve certain aspects, such as the filter precision, the local navigation reactivity, the computer vision auto-adjustment, among other details. But as a whole, the project works quite well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "72f8d028f6a762a902252b237a4fe5196d84b64f88ca64e24fcd5a5e78aa73d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
