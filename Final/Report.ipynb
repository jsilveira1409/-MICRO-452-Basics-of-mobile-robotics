{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad6287a",
   "metadata": {},
   "source": [
    "# Final project of Mobile Robotique "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d833c",
   "metadata": {},
   "source": [
    "Student group number 19 : <br/>\n",
    "Kyan Achtari <br/>\n",
    "Louis Gavignet <br/>\n",
    "Louise Genoud <br/>\n",
    "Joaquim Silveira <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a942b6",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<br/>\n",
    "The project of Mobile Robotique aims to combine vision, path planning, global navigation, local navigation, and filtering with a kalman filter to move Thymio to the goal.\n",
    "<br/><br/>\n",
    "We first create a map composed of differents elements : black obstacles where the Thymio must never go, a blue triangle on Thymio to get his position and orientation in real time, a red square goal, and white small obstacles invisible to the camera.  The camera, using the vision modulus, gets the coordinates of every elements on the map from which we extracts a path. We then use the path planning with Dijkstra to extract the best suited path. \n",
    "<br/><br/>\n",
    "While following the path, Thymio may enconters obstacles not detected by the camera. That's why it constantly scans the near environment with the IR sensors, when one is triggered Thymio avoids the obstacles using local navigation.\n",
    "<br/><br/>\n",
    "The robot also use a filter to estimate the position with the help of sensors (camera and motor speed) and to correct it. We choose to implement the kalman filter, which shows a strong efficiency even when the camera is blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a25d73",
   "metadata": {},
   "source": [
    "# Implementaion with vision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c75ed5cb",
   "metadata": {},
   "source": [
    "### Hardware and Software Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "011aaee6",
   "metadata": {},
   "source": [
    "For the computer vision implementation, we will be using the camera METTRE MODEL. The OpenCV library will be used to process and treat the images to our convenience."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d50178fc",
   "metadata": {},
   "source": [
    "### Object recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5f54720",
   "metadata": {},
   "source": [
    "The first step is to differentiate three types of objects: obstacles, the robot and the goal area. They are respectively black, blue and red, can be either squares or triangles, and be placed almost anywhere inside the frame, so long as the obstacles are twice the robot's size a part. The object recognition process for either type consists of the same 4 steps : \n",
    "\n",
    "**1. Convert Image to HSV Color Space** : from research, we found out that the Hue Saturation Value color space is ideal for edge detection, as the distinction between the colors is more evident. Also, it is less sensible to the environment's lightning, as a full Value, equivalent to brightness, corresponds to how the color behaves and appears under a certain lightning. Therefore, full brightness does not mean the color will be white, but it's own color will be more radiant.\n",
    "\n",
    "**2. Smooth the Image**: we smooth the image to reduce noise and make the pixel values inside an object more homogeneous, making it easer to establish an upper and lower color bound (in HSV of course). To smooth the image, we chose to employ the Bilateral Filter proposed by the OpenCV2 library. It is both effective for removing white noise and at the same time keeping edges sharp, which is what we'll need for the next step. Of course, it takes more computing power, but we can compensate this by reducing the image pixel width and height.\n",
    "\n",
    "**3. Image Mask**: To create the object mask, we take the lower and upper color bounds for it, which were measured using the Gimp image processing software, and use the OpenCv2 InRange function, which basically applies a threshold to the image with the given bound. If the pixel is outside of the bound, it's value is set to 0, else it will be set to 1.\n",
    "\n",
    "**4. Object Contour and Center Recognition**: Now with the object isolated in the image, we can detect it's contour and center with the OpenCv2 findContours function, which joins pixel with same values(which is why we apply thresholding, as it will work better to detect contours) and that are near by. Of course, it is highly likely that noise is still present up to this stage, which is why we apply an extra step: we approximate the contour with a simpler shape of roughly the same size, calculate it's area and reject it as a valid object if it is less than a certain threshold. This allows us to ignore smaller features that are big enough to not be picked up by the bilateral smoothing, that may be part of the arena texture. By calculating the moment of the approximated contour, we can find both it's area and center point. \n",
    "\n",
    "We now have all the informations we need for the objects: it's center, it's area and it's edge points. Each step result is shown by the following images:\n",
    "| HSV image | Smooth Image | Image Mask | Object Contour|\n",
    "\n",
    "<img src=\"images/obstacle/img_processed.png\" alt=\"drawing\" width=\"300\"/> <img src=\"images/obstacle/img_hsv.png\" alt=\"drawing\" width=\"300\"/>  <img src=\"images/obstacle/img_smooth.png\" alt=\"drawing\" width=\"300\"/>  \n",
    "<img src=\"images/obstacle/mask.png\" alt=\"drawing\" width=\"300\"/>  <img src=\"images/obstacle/frame.png\" alt=\"drawing\" width=\"300\"/>\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34559934",
   "metadata": {},
   "source": [
    "### Robot Direction Detection\n",
    "Unlike with the obstacles and the goal, we also care about the robot's direction, as we need to know it's orientation to correctly move it through the world. For this, we use an isosceles triangle pointing forward. We calculate it's edges and center as mentionned above, and can measure it's forward orienting vector by finding which vertex of it's contour is further away from the center point. This allows us to calculate it's inclination, seen from above, with respect to the x axis, or the camera's horizon. The forward pointing vector is shown in the image below:\n",
    "\n",
    "<img src=\"\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "METTRE IMAGE ICI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1915e0",
   "metadata": {},
   "source": [
    "# Path planning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b9ec8a5",
   "metadata": {},
   "source": [
    "The vision module returns a list of obstacles, characterized by the coordinates of their vertices. From these points, the path planning module does several things :\n",
    "\n",
    "- **Obstacle augmentation** : construct a series of points through which the Thymio can go, from the coordinates of the obstacle vertices\n",
    "- **Graph construction** : build a graph structure with the right connections between vertices (so that Thymio cannot cross an obstacle) and the weights of the edges initialized (they correspond to th distances between the points)\n",
    "- **Shortest path** : using Dijkstra's algorithm, find the shortest path from the start point to the end point, passing through a subset of the vertices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e5fe5ff",
   "metadata": {},
   "source": [
    "### Obstacle augmentation\n",
    "The obstacle augmentation is done by moving each vertex of each obstacle in two different ways, as illustrated in the figure below for vertex A :  \n",
    "\n",
    "<img src=obstacle.JPG width=\"400\">\n",
    "\n",
    "The point is moved in the direction parallel to each edge of the obstacle, and then in the perpendicular direction away from the obstacle. The distance of each of these translation is a constant defined as half the Thymio width plus a safety margin. This is done so that the robot will not collide with the corner of the obstacle when going around it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b37150e7",
   "metadata": {},
   "source": [
    "### Graph construction\n",
    "The construction of the graph from the augmented vertices is done in 2 steps for each obstacle :\n",
    "- The consecutive vertices are connected to each other (yellow edges)\n",
    "- The vertex of the obstacle that is the closest to a vertex of another obstacle is connected to that vertex (red edge)\n",
    "\n",
    "<img src=path_around_sev.JPG width=\"800\">  \n",
    "\n",
    "The start and end point are connected to the vertex of the graph that is the closest to them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1886c39b",
   "metadata": {},
   "source": [
    "```\n",
    "    while(len(unconnected)!=0):\n",
    "    dp = distances.copy()[unconnected]\n",
    "    idx_a = np.argmin(dp,axis=0)[0]\n",
    "    a = int(dp[idx_a,1])\n",
    "    remove(a,unconnected)\n",
    "\n",
    "    for v_b in graph[a]:\n",
    "        b = v_b[0]\n",
    "\n",
    "        w_ab = get_weight(a,b,graph)\n",
    "        if((w_ab > 0) and (distances[b,0]> distances[a,0] + w_ab)):\n",
    "            distances[b,0] = distances[a,0] + w_ab\n",
    "            predecessor[b] = a\n",
    "\n",
    "\n",
    "return distances,predecessor\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f02becac",
   "metadata": {},
   "source": [
    "From the resulting list of predecessors, the path (_coord_) can then be extracted by walking through it from end to beginning:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc7532e3",
   "metadata": {},
   "source": [
    "```\n",
    "coord.append(end)\n",
    "\n",
    "while(pred[i] != len(pred)-2):\n",
    "    i = pred[i]\n",
    "    path_num.append(i)\n",
    "    coord.append(coordlist[i][0:2])\n",
    "\n",
    "coord.append(start)\n",
    "coord.reverse()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76210b09",
   "metadata": {},
   "source": [
    "### Shortest path\n",
    "The shortest path can be computed using Dijkstra's algorithm on the graph that was constructed. First, the table of _distances_ is initialized with all distances to a value INF representing infinity, and the table of _predecessors_ is created. An _unconnected_ list is also created containing the indexes of all the vertices at the beginning.  \n",
    "\n",
    "The _graph_ is a list of lists, with every vertex represented by a list of its connected vertices and the weight of the edge connecting them. \n",
    "These tables are then updated as such :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d61ea9",
   "metadata": {},
   "source": [
    "# Motion control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c440b",
   "metadata": {},
   "source": [
    "### Global navigation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f38388c7",
   "metadata": {},
   "source": [
    "##### Implementation of the movement\n",
    "We create a fonction called pathing() that uses all the functions needed for moving the Thymio. In every iteration, pathing calls the camera, kalman, and refers to the path given by the path planning. The Thymio moves continuously and actuates the speed of his motors thanks to a proportionnal controller.\n",
    "\n",
    "##### Rotation \n",
    "When the Thymio arrives at an intermediate goal, he has to change his orientation to go to the next one. The function get_angle_between() gives the angular difference between the two vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "365f0b93",
   "metadata": {},
   "source": [
    "```\n",
    "def get_angle_between(vec1, vec2):\n",
    "    vec1_unit = vec1 / np.linalg.norm(vec1)\n",
    "    vec2_unit = vec2 / np.linalg.norm(vec2)\n",
    "\n",
    "    return np.arccos(np.dot(vec1_unit, vec2_unit))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d0a0375",
   "metadata": {},
   "source": [
    "Then, we make sure the angle belongs to the interval -pi, pi :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46032383",
   "metadata": {},
   "source": [
    "```\n",
    "def wrap_angle(angle):\n",
    "    if angle > math.pi:\n",
    "        angle = angle - 2*math.pi\n",
    "    elif angle < -math.pi:\n",
    "        angle = angle + 2*math.pi\n",
    "    return angle\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b443a5c",
   "metadata": {},
   "source": [
    "##### Controller\n",
    "\n",
    "We use a proportionnal controller to set the speed of the motors. It takes into account the angle for the correcting or new orientation. We also set an angle minimum under which the robot won't turn, the kalman correction can be very precice so we don't take it in consideration every time. This angle minimum is equal to 0.4 radians, which corresponds to 23 degrees. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "026e1d23",
   "metadata": {},
   "source": [
    "```\n",
    "def controller(angle)\n",
    "    if abs(angle) > ANGLE_TOLERANCE:\n",
    "                speed_l = 80 - kp_rot*(angle)\n",
    "                speed_r = 80 + kp_rot*(angle)\n",
    "            else:\n",
    "                speed_l = SPEED_AVG\n",
    "                speed_r = SPEED_AVG\n",
    "    return speed_l, speed_r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce9d03",
   "metadata": {},
   "source": [
    "### Local navigation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a4665ca",
   "metadata": {},
   "source": [
    "We read the 7 horizontal sensor values and store them in prox.vals and multiply them by IR.SCALE.DOWN (which is 0.05), if any of the values is non-zero (other than the last 2, which are behind the Thymio), we enter obstacle avoidance:\n",
    "\n",
    "Then we multiply the vector prox.vals by LEFT.IR.WEIGHT and RIGHT.IR.WEIGHT for each wheel:\n",
    "\n",
    "$$\n",
    "LEFT.IR.WEIGHT =\n",
    "\\left(\\begin{array}{cc} \n",
    "1 \\\\\n",
    "2 \\\\\n",
    "-5 \\\\\n",
    "-2 \\\\\n",
    "-1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{array}\\right)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "RIGHT.IR.WEIGHT =\n",
    "\\left(\\begin{array}{cc} \n",
    "-1 \\\\\n",
    "-2 \\\\\n",
    "-5 \\\\\n",
    "2 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{array}\\right)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "$$\n",
    "\n",
    "The speed of each wheel is their respective weight multiplied by the sensor values, and + 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "977a2ee8",
   "metadata": {},
   "source": [
    "```\n",
    "if sum(prox_vals[0], prox_vals[1], prox_vals[2], prox_vals[3], prox_vals[4]) > 0:\n",
    "    obst = True\n",
    "\n",
    "if obst == True:\n",
    "    speed_l = sum(prox_vals * LEFT_IR_WEIGHT)  + 10\n",
    "    speed_r = sum(prox_vals * RIGHT_IR_WEIGHT) + 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f10df",
   "metadata": {},
   "source": [
    "# Filtering with Kalman filter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d5ab6",
   "metadata": {},
   "source": [
    "### Overview and prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57df6b7a",
   "metadata": {},
   "source": [
    "To calculate the beliefs of the robot position we setup a Bayes filter. It's a recrsive filter taht calculte first a prediction of the robot positioon and then update the measurement. We implemented an extended Kalman filter because we conside a nonlinear gaussian state space model, our state are $X = (x,y, \\theta)$. The states are modified by the speed of the left and right motors U = (speed_left, spped_right). The nonlinear modification of each paramters can be modelled like this : \n",
    "$$ x_{t+1} = x_{t} + v_{average}\\cdot dt\\cdot cos(\\theta) $$ \n",
    "$$ y_{t+1} = y_{t} + v_{average} \\cdot dt\\cdot sin(\\theta) $$ \n",
    "$$ \\theta_{t+1} = \\theta_{t} + dt\\cdot \\omega $$  \n",
    "\n",
    "<c/><c/>\n",
    "We can write the systeme above with some matrices :\n",
    "$$ X_{t+1} = A\\cdot X_{t} + B\\cdot U $$\n",
    "\n",
    "<c/><c/>\n",
    "With the following matrices A and B :\n",
    "$$\n",
    "A = \n",
    "\\left(\\begin{array}{cc} \n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "B = \n",
    "\\left(\\begin{array}{cc} \n",
    "0.5\\cdot dt\\cdot cos(\\theta) & 0.5 \\cdot dt\\cdot cos(\\theta)\\\\\n",
    "0.5\\cdot dt \\cdot sin(\\theta) & 0.5\\cdot dt\\cdot sin(\\theta) \\\\\n",
    "-dt/wheeldist & dt/wheeeldist \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101833ef",
   "metadata": {},
   "source": [
    "The state transition covariance matrix Q, contains the variance of the motors speed :\n",
    "\n",
    "$$ \n",
    "Q = \n",
    "\\left(\\begin{array}{cc}\n",
    "speedvar & 0\\\\\n",
    "0 & speedvar\\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "The covariance of the obervation noise matrix R, errors come from the camera when measuring a known object :\n",
    "\n",
    "$$ \n",
    "R = \n",
    "\\left(\\begin{array}{cc}\n",
    "camvar & 0 & 0\\\\\n",
    "0 & camvar & 0\\\\\n",
    "0 & 0 & camanglevar\\\\\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f788486",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efdfc15a",
   "metadata": {},
   "source": [
    "We can predict the next position of the robot thanks to the set of equations of the nonlinear $X_{t+1}$. We calculate the predication every time interval dt, this periode corresponds to actuation time of our robot. Every dt we calculate the new orientation to follow the path, we update the speed of the motors, and we check with the Kalman filter that Thymio is on the right path.\n",
    "\n",
    "We also compute the prediction of an estimate of the covariance in this function :\n",
    "$P = P+B\\cdot Q\\cdot B^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a9d52",
   "metadata": {},
   "source": [
    "### Update "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc0b72f7",
   "metadata": {},
   "source": [
    "Update the Kalman filter state estimate previously with the Kalman gain K, and the difference between the actual state z and the predicted one x. In the mean time, we update the values of P.\n",
    "\n",
    "$$I = z-X$$\n",
    "and $$K = P \\cdot H^T\\cdot S$$  with $$S=R+H\\cdot P^T\\cdot H$$\n",
    "then $$X=X+K\\cdot I$$\n",
    "and $$P = P-K\\cdot H\\cdot P$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26779ae7",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729027b0",
   "metadata": {},
   "source": [
    "We then implement the filter function by calling the prediction function and the update function in this order every dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e58a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f9566ee846e0e5475f3731207e71ee4a96d604221359f666805a9fa43f54da3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
